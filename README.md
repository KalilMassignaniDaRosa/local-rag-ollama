
# ğŸš€ Advanced Local RAG System

A sophisticated **Retrieval-Augmented Generation (RAG)** system that runs entirely on your local machine using **Ollama**, **PostgreSQL with pgvector**, and **advanced document processing**.

---

## ğŸš€ Quick Start

### Prerequisites
- Docker and Docker Compose  
- Python **3.8+**  
- Ollama (with at least one model installed)

---

## 1. Clone and Setup

```bash
git clone <repository-url>
cd rag-local
```

---

## 2. Start Services with Docker

```bash
docker-compose up -d
```

This starts:

- **PostgreSQL with pgvector** on port **5433**  
- **Ollama** on port **11434**

---

## 3. Install Python Dependencies

```bash
pip install -r requirements.txt
```

---

## 4. Pull Required Models

```bash
# Pull LLM model
ollama pull llama3.2:3b
```

---

## 5. Run the System

```bash
python cli.py
```

---

# ğŸ”„ Basic Workflow

- **Place Documents:** Add PDF files to the `documents/` folder  
- **Ingest Documents:** Use option **1** in the CLI to load documents  
- **Process & Index:** Use option **2** to create chunks and embeddings  
- **Query:** Use options **3** or **4** to ask questions  

---

# ğŸ§  Advanced Query Features

## Template Types
- `query` â€” Standard query with precise citations  
- `summary` â€” Structured document summarization  
- `comparison` â€” Comparative analysis between documents  
- `extraction` â€” Specific information extraction  
- `qa` â€” Optimized question-answering  

## Output Formats
- `text` â€” Human-readable text format  
- `json` â€” Structured JSON response  
- `markdown` â€” Markdown formatted response

---

# âš™ï¸ Performance Tuning

## Embedding Generation
- Use **all-MiniLM-L6-v2** for a balance of speed and quality.  
- Consider **paraphrase-multilingual-MiniLM-L12-v2** for multilingual content.

## Chunking Strategy
- **Fixed:** Fastest, good for uniform documents.  
- **Semantic:** Better for structured documents with clear paragraphs.  
- **Sentence:** Best for question-answer pairs and precise retrieval.

## Search Parameters
- **Threshold 0.2â€“0.3:** Higher recall, more results.  
- **Threshold 0.4â€“0.6:** Balanced precision and recall.  
- **Threshold 0.7+:** High precision, fewer results.

---

# ğŸ§© Configuration & Files
- `docker-compose.yml` â€” service definitions for PostgreSQL + pgvector and Ollama  
- `requirements.txt` â€” Python dependencies  
- `cli.py` â€” command-line interface for ingestion and querying  
- `documents/` â€” place PDFs here for ingestion  
- Database: PostgreSQL with `pgvector` extension (port **5433**)  
- Ollama API: default **http://localhost:11434**

---

# ğŸ™ Acknowledgments
- **Ollama** â€” local LLM hosting  
- **PostgreSQL & pgvector** â€” efficient vector storage and search  
- **Sentence Transformers** â€” embedding models  
- **PyPDF2** â€” PDF text extraction